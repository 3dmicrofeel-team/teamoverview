<div align="center">


<font size=4> README: EN | <a href="./README.zh.md">‰∏≠Êñá</a>  </font>
    <h1>3dmicrofeel üòä: TEAM OVERVIEW</h1>
</div>


<br>

**Microfeel** is an interdisciplinary team with excellent overseas PhD academic background. We have been deeply engaged in the fields of spatial automatic generation and spatial understanding, artificial intelligence and machine learning for many years. We are the earliest group of researchers and practitioners to conduct AI3D.

---
## Ximing Zhong(CEO)

1.**[A FRAMEWORK FOR FINE-TUNING URBAN GANS USING DESIGN DECISION DATA GENERATED BY ARCHITECTS THROUGH GANS APPLICATIONS](https://research.aalto.fi/en/publications/a-framework-for-fine-tuning-urban-gans-using-design-decision-data)**

**Ximing Zhong, Jiadong Liang, Pia Fricker, Shengyu Liu**

*Abstract*

Recent studies have utilized Generative Adversarial Networks (GANs) to learn from existing urban layouts for urban design tasks. We define these GANs as Urban-GAN. However, urban layouts generated by Urban-GAN lack specificity and often require multiple modifications by architects to meet specific design requirements, making the process inefficient and non-customizable. Inspired by the concept of fine-tuning language models, we propose a stacked GAN model framework that fine-tunes Urban-GAN using data generated by architects in solving specific design tasks, forming AD-Urban-GAN. Our results indicate that layouts produced by AD-Urban-GAN more effectively emulate architects' design morphology decisions, enhancing Urban-GAN‚Äôs adaptability and efficiency in handling design tasks. Furthermore, AD-Urban-GAN enhances the customizability of Urban-GAN models for specific urban design tasks, generating layouts that accurately understand and meet the requirements of specific tasks. AD Urban-GAN significantly streamlines the process of generating design prototypes for specific task types, enabling precise quantitative control over urban layout results. This workflow establishes a data acquisition and training loop that strengthens the customizability of existing GANs. The design decision data generated by architects can improve the adaptability and customization of GANs models, facilitating efficient collaborative work between architects and artificial intelligence.

2.**[AI as a Collaborative Partner in Landscape Form-finding](https://research.aalto.fi/en/publications/ai-as-a-collaborative-partner-in-landscape-form-finding)**

**Chuheng Tan, Ximing Zhong, Prof. Dr. Pia Fricker**

*Abstract*

This study introduces an AI-assisted workflow for wind simulation in landscape form-finding. It can rapidly deliver a series of design options within designers' predefined constraints, each detailed with wind indicators. Integrating AI to detect subtle environmental changes and align with designers' intuitive decisions, this research fosters a collaborative paradigm between landscape architects and AI, aiming to shift from physics engine simulations to employing real-time AI simulations for rapidly aiding designers in the form-finding process in landscape design.

3.**[BRIDGING BIM AND AI: A Graph-BIM Encoding Approach For Detailed 3D Layout Generation Using Variational Graph Autoencoder](https://research.aalto.fi/en/publications/bridging-bim-and-ai-a-graph-bim-encoding-approach-for-detailed-3d)**

**Jiadong Liang, Ximing Zhong, Immanuel Koh**

*Abstract*

Building Information Modelling (BIM) data provides an abundant source with hierarchical and detailed information on architectural elements. Nevertheless, transforming BIM data into an understandable format for AI to learn and generate controllable and detailed three-dimensional (3D) models remains a significant research challenge. This paper explores an encoding approach for converting BIM data into graph-structured data for AI to learn 3D models, which we define as Graph-BIM encoding. We employ the graph reconstruction capabilities of a Variational Graph Autoencoder (VGAE) for the unsupervised learning of BIM data to identify a suitable encoding method. vGaE's graph generation capabilities also reason for spatial layouts. Results demonstrate that VGAE can reconstruct BIM 3D models with high accuracy, and can reason the entire spatial layout from partial layout information detailed with architectural components. The primary contribution of this research is to provide a novel encoding approach for bridging AI and BIM encoding. The Graph-BIM encoding method enables low-cost, self-supervised learning of diverse BIM data, capable of learning and understanding the complex relationships between architectural elements. Graph-BIM provides foundational encoding for training general-purpose AI models for 3D generation.

4.**[A Human‚ÄìMachine Collaborative Building Spatial Layout Workflow Based on Spatial Adjacency Simulation](https://research.aalto.fi/en/publications/a-humanmachine-collaborative-building-spatial-layout-workflow-bas)**

**Ximing Zhong, Fujia Yu, Beichen Xu**

*Abstract*

The space layout of a reasonable modular building prototype is a time consuming and complex process. Many studies have optimised automatic spatial layouts based on spatial adjacency simulation. Although machine-produced plans satisfy the adjacency and area constraints, people still need further manual modifications to meet other spatially complex design requirements. Motivated by this, we provide a human‚Äìmachine collaborative design workflow that simulates the spatial adjacency relationship based on physical models. Compared with previous works, our workflow enhances the automated space layout process by allowing designers to use environment anchors to make decisions in automatic layout iterations. A case study is proposed to demonstrate that the solution generated by our workflow can initially complete different customised design tasks. The workflow combines the advantages of the designer's decision-making experience in manual modelling with the machine's ability in rapid automated layout. In the future, it has the potential to be developed into a designer-machine collaboration tool for completing complex building design tasks.

5.**[A Rapid Wind Velocity Prediction Method in Built Environment Based on CycleGAN Model](https://research.aalto.fi/en/publications/a-rapid-wind-velocity-prediction-method-in-built-environment-base)**

**Chuheng Tan, Ximing Zhong**

*Abstract*

Although the wind microclimate and wind environment play important roles in urban prediction, the time-consuming and complicated setup and process of wind simulation are widely regarded as challenges. There are several methods to use deep learning (DL) models for wind speed prediction by labeling pairs of wind simulation dataset samples. However, many wind simulation experiments are needed to obtain paired datasets, which is still time-consuming and cumbersome. Compared with previous studies, we propose a method to train a DL model without labelling paired data, which is based on Cycle Generative Adversarial Network (cycleGAN). To verify our hypothesis, we evaluate the results and process of the pix2pix model (requires paired datasets) and cycleGAN (does not requires paired datasets), and explore the difference of results between these two DL models and professional CFD software. The result shows that cycleGAN can perform as well as pix2pix in accuracy, indicating that some random city plans image samples and random wind simulation samples can train surrogate models as accurate as labelled DL methods. Although the DL method has similar results to the professional CFD method, the details of the wind flow results still need improvement. This study can help designers and policymakers to make informed decisions to choose Dl methods for real-time wind speed prediction for early-stage design exploration.


6.**[Building-GNN: Exploring a co-design framework for generating controllable 3D building prototypes by graph and recurrent neural networks](https://research.aalto.fi/en/publications/building-gnn-exploring-a-co-design-framework-for-generating-contr)**

**Ximing Zhong, Immanuel Koh, Prof. Dr. Pia Fricker**

*Abstract*

This paper discusses a novel deep learning (DLÔºâframework named Building-GNN, which combines the Graph Neural Network (GNN) and the Recurrent neural network (RNN) to address the challenge of generating a controllable 3D voxel building model. The aim is to enable architects and AI to jointly explore the shape and internal spatial planning of 3D building models, forming a co-design paradigm. While the 3D results of previous DL methods, such as 3DGAN, are challenging to control in detail and meet the constraints and preferences of architects' inputs, Building-GNN allows for reasoning about the complex constraint relationships between each voxel. In Building-GNN, the GNN simulates and learns the graph structure relationship between 3D voxels, and the RNN captures the complex interplaying constraint relationships between voxels. The training set consists of 4000 rule-based generated 3D voxel models labeled with different degrees of masking. The quality of the 3D results is evaluated using metrics such as IoU, Fid, and constraint satisfaction. The results demonstrate that adding RNN enhances the accuracy of 3D model shape and voxel relationship prediction. Building-GNN can perform multi-step rational reasoning to complete the 3D model layout planning in different scenarios based on the architect's precise control and incomplete input.

7.**[A Discussion on an Urban Layout Workflow Utilizing Generative Adversarial Network (GAN) - With a focus on automatized labeling and dataset acquisition](https://research.aalto.fi/en/publications/a-discussion-on-an-urban-layout-workflow-utilizing-generative-adv)**

**Ximing Zhong, Pia Fricker, Fujia Yu, Chuheng Tan, Yuzhe Pan**

*Abstract*

Deep Learning (DL) has recently gained widespread attention in the automation of urban layout processes. This study proposes a rule-based and Generative Adversarial Network (GAN) workflow to automatically select and label urban datasets to train customized GAN models for the generation of urban layout proposals. The developed workflow automatically collects and labels urban typology samples from open-source maps. Furthermore, it controls the results of the GAN process with labels and provides real-time urban layout suggestions based on a co-design process. The conducted case study shows that the average value of the GAN results, trained from an automatically generated dataset, meets the site's requirements. The developed co-design strategy allows the architect to control the GAN process and perform iterations on urban layouts. The research addresses the research gap in GAN applications in the field of urban design and planning. Many studies have demonstrated that training the (GAN) model by labeling enables machines to learn urban morphological features and urban layout logic. However, two research gaps remain: (1) The manual filtering of GAN urban sample datasets to fit site-specific design requirements is very time-consuming. (2) Without a suitable data labeling method, it is difficult to manage the GAN process in such a manner to facilitate the meeting of overriding design requirements.

8.**[GaussianSpace: Text guide 3D Gaussian](https://gaussianspace.github.io/)**

**Shengyu Meng, Ximing Zhong, Fujia Yu**

*Abstract*

Integrating 2D diffusion models with 3D Gaussian splatting for text-guided generation of individual 3D objects and editing Gaussian scenes are recently receiving increasing attention. However, current methods for single object generation normally require additional constraints, such as masks and normal maps, which limit their applicability in handling complex spaces. Furthermore, current techniques for text-guided 3D Gaussian editing typically rely on Iterative Dataset Update (IDU) methods based on the instruct nerf2nerf, which are also unsuitable for large spatial manipulations. In response to these challenges, we propose a new method, GaussianSpace, which enables effective text-guided editing of large space in 3D Gaussian Splatting. The key innovation of this method lies in its consideration of both RGB loss from the ground-true images and Score Distillation Sampling (SDS) loss based on the diffusion model during the iterative process. Additionally, we have introduced an automatic weighted loss technique to steadily descend the overall loss, ensuring that the edited Gaussian scene adapts to text instructions while retaining its original structural features. This marks the first successful implementation of text-guided 3D Gaussian large spatial manipulations.

## Fujia Yu

1.**[]()**

****

**


2.**[]()**

****

**






















## Demo

A demo is hosted on Replicate, [EmotiVoice](https://replicate.com/bramhooimeijer/emotivoice).

## Hot News

- [x] Tuning voice speed is now supported in 'OpenAI-compatible-TTS API', thanks to [@john9405](https://github.com/john9405). [#90](https://github.com/netease-youdao/EmotiVoice/pull/90) [#67](https://github.com/netease-youdao/EmotiVoice/issues/67) [#77](https://github.com/netease-youdao/EmotiVoice/issues/77)

- [x] [The EmotiVoice app for Mac](https://github.com/netease-youdao/EmotiVoice/releases/download/v0.3/emotivoice-1.0.0-arm64.dmg) was released on December 28th, 2023. Just download and taste EmotiVoice's offerings!

- [x] [The EmotiVoice HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) was released on December 6th, 2023. Easier to start, faster to use, and with **over 13,000 free calls**. Additionally, users can explore more captivating voices provided by [Zhiyun](https://ai.youdao.com/).
- [x] [Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023, along with [DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker) and [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech). 

## Features under development

- [ ] Support for more languages, such as Japanese and Korean. [#19](https://github.com/netease-youdao/EmotiVoice/issues/19) [#22](https://github.com/netease-youdao/EmotiVoice/issues/22)

EmotiVoice prioritizes community input and user requests. We welcome your feedback!

## Quickstart

### EmotiVoice Docker image

The easiest way to try EmotiVoice is by running the docker image. You need a machine with a NVidia GPU. If you have not done so, set up NVidia container toolkit by following the instructions for [Linux](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2) or [Windows WSL2](https://github.com/nyp-sit/it3103/blob/main/nvidia-docker-wsl2.md). Then EmotiVoice can be run with,

```sh
docker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest
```
The Docker image was updated on January 4th, 2024. If you have an older version, please update it by running the following commands:
```sh
docker pull syq163/emoti-voice:latest
docker run -dp 127.0.0.1:8501:8501 -p 127.0.0.1:8000:8000 syq163/emoti-voice:latest
```
Now open your browser and navigate to http://localhost:8501 to start using EmotiVoice's powerful TTS capabilities.

Starting from this version, the 'OpenAI-compatible-TTS API' is now accessible via http://localhost:8000/.

### Full installation

```sh
conda create -n EmotiVoice python=3.8 -y
conda activate EmotiVoice
pip install torch torchaudio
pip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict
python -m nltk.downloader "averaged_perceptron_tagger_eng"
```

### Prepare model files

We recommend that users refer to the wiki page [How to download the pretrained model files](https://github.com/netease-youdao/EmotiVoice/wiki/Pretrained-models) if they encounter any issues.

```sh
git lfs install
git lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese
```
or, you can run:
```sh
git clone https://www.modelscope.cn/syq163/WangZeJun.git
```

### Inference

1. You can download the [pretrained models](https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing) by simply running the following command:
```sh
git clone https://www.modelscope.cn/syq163/outputs.git
```
2. The inference text format is `<speaker>|<style_prompt/emotion_prompt/content>|<phoneme>|<content>`. 

  - inference text example: `8051|Happy|<sos/eos> [IH0] [M] [AA1] [T] engsp4 [V] [OY1] [S] engsp4 [AH0] engsp1 [M] [AH1] [L] [T] [IY0] engsp4 [V] [OY1] [S] engsp1 [AE1] [N] [D] engsp1 [P] [R] [AA1] [M] [P] [T] engsp4 [K] [AH0] [N] [T] [R] [OW1] [L] [D] engsp1 [T] [IY1] engsp4 [T] [IY1] engsp4 [EH1] [S] engsp1 [EH1] [N] [JH] [AH0] [N] . <sos/eos>|Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine`.
4. You can get phonemes by `python frontend.py data/my_text.txt > data/my_text_for_tts.txt`.

5. Then run:
```sh
TEXT=data/inference/text
python inference_am_vocoder_joint.py \
--logdir prompt_tts_open_source_joint \
--config_folder config/joint \
--checkpoint g_00140000 \
--test_file $TEXT
```
the synthesized speech is under `outputs/prompt_tts_open_source_joint/test_audio`.

1. Or if you just want to use the interactive TTS demo page, run:
```sh
pip install streamlit
streamlit run demo_page.py
```

### OpenAI-compatible-TTS API

Thanks to @lewangdev for adding an OpenAI compatible API [#60](../../issues/60). To set it up, use the following command:

```sh
pip install fastapi pydub uvicorn[standard] pyrubberband
uvicorn openaiapi:app --reload
```

### Wiki page

You may find more information from our [wiki](https://github.com/netease-youdao/EmotiVoice/wiki) page.

## Training

[Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023.


## Roadmap & Future work

- Our future plan can be found in the [ROADMAP](./ROADMAP.md) file.
- The current implementation focuses on emotion/style control by prompts. It uses only pitch, speed, energy, and emotion as style factors, and does not use gender. But it is not complicated to change it to style/timbre control.
- Suggestions are welcome. You can file issues or [@ydopensource](https://twitter.com/YDopensource) on twitter.


## WeChat group
Welcome to scan the QR code below and join the WeChat group.

<img src="https://github.com/netease-youdao/EmotiVoice/assets/49354974/cc3f4c8b-8369-4e50-89cc-e40d27a6bdeb" alt="qr" width="150"/>

## Credits

- [PromptTTS](https://speechresearch.github.io/prompttts/). The PromptTTS paper is a key basis of this project.
- [LibriTTS](https://www.openslr.org/60/). The LibriTTS dataset is used in training of EmotiVoice.
- [HiFiTTS](https://www.openslr.org/109/). The HiFi TTS dataset is used in training of EmotiVoice.
- [ESPnet](https://github.com/espnet/espnet). 
- [WeTTS](https://github.com/wenet-e2e/wetts)
- [HiFi-GAN](https://github.com/jik876/hifi-gan)
- [Transformers](https://github.com/huggingface/transformers)
- [tacotron](https://github.com/keithito/tacotron)
- [KAN-TTS](https://github.com/alibaba-damo-academy/KAN-TTS)
- [StyleTTS](https://github.com/yl4579/StyleTTS)
- [Simbert](https://github.com/ZhuiyiTechnology/simbert)
- [cn2an](https://github.com/Ailln/cn2an). EmotiVoice incorporates cn2an for number processing.

## License

EmotiVoice is provided under the Apache-2.0 License - see the [LICENSE](./LICENSE) file for details.

The interactive page is provided under the [User Agreement](./EmotiVoice_UserAgreement_ÊòìÈ≠îÂ£∞Áî®Êà∑ÂçèËÆÆ.pdf) file.
